{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution is available in the other \"quiz_solution.py\" tab\n",
    "import tensorflow as tf\n",
    "\n",
    "def get_weights(n_features, n_labels):\n",
    "    return tf.Variable(tf.truncated_normal((n_features, n_labels)))\n",
    "    \"\"\"\n",
    "    Return TensorFlow weights\n",
    "    :param n_features: Number of features\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow weights\n",
    "    \"\"\"\n",
    "    # TODO: Return weights\n",
    "\n",
    "\n",
    "def get_biases(n_labels):\n",
    "    \"\"\"\n",
    "    Return TensorFlow bias\n",
    "    :param n_labels: Number of labels\n",
    "    :return: TensorFlow bias\n",
    "    \"\"\"\n",
    "    # TODO: Return biases\n",
    "    return tf.Variable(tf.truncated_normal((1, n_labels)))\n",
    "\n",
    "\n",
    "def linear(inp, w, b):\n",
    "    \"\"\"\n",
    "    Return linear function in TensorFlow\n",
    "    :param input: TensorFlow input\n",
    "    :param w: TensorFlow weights\n",
    "    :param b: TensorFlow biases\n",
    "    :return: TensorFlow linear function\n",
    "    \"\"\"\n",
    "    # TODO: Linear Function (xW + b)\n",
    "    tf.matmul(inp, w) + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5163807   0.4510541   1.1346564   0.4489646   1.3375736 ]\n",
      " [ 0.05721911 -1.991419    0.43748114  1.6802514  -0.5634286 ]\n",
      " [ 1.771018    0.5016466  -0.3762916   1.0609574  -1.1152768 ]\n",
      " [ 1.1660752  -1.5122176   1.1826799   0.92978543 -1.5718247 ]\n",
      " [-1.5402457   1.4142694   1.2527646   0.5487784   0.35509375]\n",
      " [-0.93930084  1.4226005   0.2838536  -0.165643   -0.04480771]\n",
      " [ 0.8290549  -0.44950503  0.14909142  0.61601967 -0.9581105 ]\n",
      " [ 0.76694983 -1.3022965  -0.8575185   1.856465   -0.9739994 ]\n",
      " [ 0.7307002   0.31903377  1.6695223   1.0460382   0.59988797]\n",
      " [ 0.2794839   0.6773302  -1.2584665  -1.0284104   0.7230468 ]\n",
      " [ 0.8334647   0.6525321   0.17526813  0.9573377  -0.9439719 ]\n",
      " [ 0.3183377   1.619714    0.02756759 -1.5794415  -0.5931467 ]\n",
      " [ 0.14822346  1.1591914  -0.3545326   0.9252893   0.96005267]\n",
      " [-1.2826897  -0.2369574   0.17072923 -0.4911093  -1.0656211 ]\n",
      " [ 1.0796279  -0.6949805  -0.9011509   1.261957    0.32211846]\n",
      " [ 0.44941956  0.2651654   0.22392559  0.12572828  1.9374387 ]\n",
      " [-0.12516242  0.86630344  1.4201032  -0.10291809  0.2115433 ]\n",
      " [ 0.30189854 -1.1678749   0.9931292   0.26954538  0.38660017]\n",
      " [ 1.4804735  -0.6479313  -0.29575884 -1.7949541  -0.62427235]\n",
      " [ 0.7292751  -0.20178197 -0.85626304 -0.5467442  -0.2424937 ]\n",
      " [ 1.0340173  -0.5570118   0.53396064  1.3609469   0.6183225 ]\n",
      " [ 0.6991925   1.0720536  -1.1494477  -1.7810178   0.65581095]\n",
      " [-0.62118375 -0.33097583  0.99718434  0.15922105 -1.9563236 ]\n",
      " [ 1.3059442   0.62988454  0.02838805  1.1469907  -0.8977204 ]\n",
      " [-0.74292576  1.084904    0.09442063  0.52054805 -0.56845784]\n",
      " [-0.23602536  0.10857779 -0.552617   -0.39304927 -0.9517063 ]\n",
      " [-0.96055484  0.1695462   0.12555483 -0.23210162  0.5070898 ]\n",
      " [-0.3059897   0.908771    0.29574546  1.3932077  -0.82602316]\n",
      " [-0.30589753 -1.8855044   0.2157901  -0.74548477 -0.55126584]\n",
      " [-0.95880115 -0.00989254 -1.5962448   0.03361804  0.36098027]\n",
      " [ 1.2059888   0.24921317 -0.19035715 -0.56601095  1.8267263 ]\n",
      " [ 0.7768      1.1392341   1.0557203  -0.43653062  0.6624354 ]\n",
      " [-0.54269177  1.4508932   0.5239053   1.5277593   0.7920249 ]\n",
      " [-0.08486369 -0.69465595 -1.8531064   1.7533998   1.7149168 ]\n",
      " [-1.0999316   0.1674828  -0.07833844 -0.9822113   0.9187997 ]\n",
      " [-0.5477947   0.8774962  -0.30319405 -1.9095782  -0.24535921]\n",
      " [-0.9003721   0.11951014 -0.43109497  0.9055809   0.32705206]\n",
      " [-0.94793445  0.20185637  1.3223213  -0.8405044   1.8914152 ]\n",
      " [-0.3101712   1.7144195  -0.76612     1.8885889   0.2658601 ]\n",
      " [ 1.3818524   0.14318699 -0.17362113 -0.8322001   0.7173268 ]\n",
      " [-0.79710907  0.42264748  1.045962   -0.9760996  -0.19578794]\n",
      " [ 1.0149359  -0.71604496  0.77620184  0.20696399 -0.6051828 ]\n",
      " [-0.96223676 -0.6479686   0.9738032  -1.3984637   0.09241614]\n",
      " [ 0.519317    0.5181157  -1.355103   -1.834792   -0.19709995]\n",
      " [ 0.29435396 -0.931381   -1.2447197  -0.0031914   0.83149   ]\n",
      " [ 0.5245731  -1.8682766   0.22784805  0.9464265   0.25996417]\n",
      " [-0.6664902  -0.7509436   0.7790703   0.69611406 -0.08867953]\n",
      " [-0.72089195 -0.48314407  0.667277    0.5949325   1.1239599 ]\n",
      " [ 0.9741841  -0.7300215   0.90148336 -1.4655279  -1.3332665 ]\n",
      " [ 1.6787491   0.2667309  -0.71127707 -0.14423956 -1.9492996 ]\n",
      " [-1.2196399  -1.6997559  -0.6996604   0.27100018  0.6777089 ]\n",
      " [ 1.2371885   1.4014477  -1.5255275   0.16323219  0.3874506 ]\n",
      " [-0.9694824  -1.0306275   0.03843882  1.6642371   0.85730994]\n",
      " [ 0.7583051   0.12789328 -0.04810869  1.7905965  -0.78769857]\n",
      " [-1.0046624   1.6843295  -1.3151339   0.6129355  -0.0949516 ]\n",
      " [ 1.1259125   0.7618458   1.3926997  -0.20518486  1.0717269 ]\n",
      " [-0.13931912 -0.4757405  -0.48042622 -0.42211723  0.8739807 ]\n",
      " [ 0.09148047 -1.3757292   0.5325948  -0.6652296   1.6507955 ]\n",
      " [-0.21009916  0.18385178  0.5196689  -0.39040843 -0.4258804 ]\n",
      " [ 0.4678333   0.7592385   1.5938472   1.0219916   1.1523867 ]\n",
      " [ 0.97639203 -0.07527604  0.5840961   0.10093344  0.7244657 ]\n",
      " [ 0.49973872 -0.20612895  1.1516328  -0.46703526 -1.0062636 ]\n",
      " [ 0.02738093  0.12289007  1.7070441  -0.6211399  -0.37841013]\n",
      " [ 0.2343657  -0.103658   -1.0886755  -1.8252726   0.7381166 ]\n",
      " [-0.42540777  1.4206111  -0.36162397  0.22177473  1.006856  ]\n",
      " [ 1.1056439  -1.0884322  -0.03634435  0.38980204 -0.47186917]\n",
      " [ 0.93211687 -0.9748396   1.2356803  -0.7716561   1.6090524 ]\n",
      " [-1.2971386   1.5676433   0.40943325  0.14488178 -0.61951256]\n",
      " [ 0.00577485  0.26179078  1.9460628   1.0981797  -0.13949648]\n",
      " [ 0.78938806  0.31168288  0.00273347  0.18752657 -1.580614  ]\n",
      " [-0.50613636  0.8870665  -1.2497193  -0.62137485 -0.56733984]\n",
      " [ 0.0496069   0.04932382 -0.7377041  -0.7904598  -0.20275962]\n",
      " [ 0.6522369   0.8931549   0.14009608  0.40999457 -0.56235963]\n",
      " [-1.083833   -0.13563229 -0.84338343 -0.08167697  0.5515409 ]\n",
      " [ 0.4265957  -0.67064595  0.42576674 -0.33235994  0.84498227]\n",
      " [ 0.24981576 -1.3312168  -1.5148386   0.42209542 -1.7577126 ]\n",
      " [ 1.4888284  -1.8399779  -0.9566931  -0.9556834   0.34167382]\n",
      " [-0.2156157   0.00395015  0.08168671 -0.5584397   0.14737417]\n",
      " [-0.6922856  -0.05577268  0.8555493  -0.34088713  0.44367838]\n",
      " [ 0.289461   -0.09068303 -1.4351219   0.92505616  0.93211156]\n",
      " [ 0.6616409   0.6392032   0.9997778  -0.14912054  0.68825316]\n",
      " [-0.3397273   0.702492    0.4374819  -1.5964241  -0.05102386]\n",
      " [ 0.32974726  0.8688128   0.14848252 -1.3307875   0.7604138 ]\n",
      " [-0.5393249  -0.5731857  -0.4418946   0.12613058  0.3940013 ]\n",
      " [ 1.094261    0.97327995  0.08317282  0.31746295  0.27926725]\n",
      " [-0.20197552  1.1210802  -1.7406904  -0.15488455  0.32342842]\n",
      " [ 0.6709248   0.8824405  -0.5236143   0.85794824 -0.0068113 ]\n",
      " [ 0.00363479  0.01513355 -0.34471336 -0.553092   -1.6335313 ]\n",
      " [-0.24181832 -0.5874606  -0.38422087  1.0608568  -0.5617944 ]\n",
      " [ 0.8362942   1.4547491   0.4719341   0.6114117   0.5329005 ]\n",
      " [-0.12873602 -0.06177362 -1.8583117  -0.7120672   0.35179242]\n",
      " [ 0.10345744 -1.2090098   0.3502668   1.4119657  -0.9863631 ]\n",
      " [-0.93765295 -0.7707526   0.53746414  1.4975519  -0.58487684]\n",
      " [-0.9484204  -0.7683816  -1.535614   -0.98539245  0.09117103]\n",
      " [-0.77656376  1.6439759  -1.4371583   0.67028624  0.12453409]\n",
      " [ 1.5533112  -0.27126738 -1.0027046   0.24330033  0.3673067 ]\n",
      " [ 0.12009758 -1.7594699  -1.3882195   1.1428396  -0.6360035 ]\n",
      " [-0.7565251  -1.133086    0.50616395  0.01781756 -1.2103747 ]\n",
      " [ 1.3793952  -1.5566894   1.3523788  -0.7236131   0.80135435]\n",
      " [-0.41891378  1.1075221   0.7424248  -1.5883482   0.65318143]\n",
      " [-1.7333688  -0.7671567   0.8881849   0.07829043  0.61981136]\n",
      " [-0.06704208 -0.1743542  -0.55846494  1.0253781  -0.05112112]\n",
      " [-1.907937    0.48871842  0.3165679   0.6070624   0.46363932]\n",
      " [ 0.92596126  1.0262256  -1.1944877  -0.45488423 -0.68244636]\n",
      " [ 1.7768054   1.7721481   0.68756866 -0.87718403 -0.51772785]\n",
      " [ 0.6704425  -0.47542167  0.8658662   1.1194057  -0.87167853]\n",
      " [ 1.0894978  -0.9664839   0.11815078  0.6519219   0.30273142]\n",
      " [-0.2438388   0.18248777 -0.81841403  0.8874255   0.2471215 ]\n",
      " [ 1.4070266   0.15705197 -1.1672387  -0.09660533 -0.78063774]\n",
      " [-1.5838625   0.5661415   0.60353494  1.1108936   0.33867306]\n",
      " [-1.5267963  -0.4658735   0.68351907  1.1459515   0.60591537]\n",
      " [ 0.8709748   1.1388808  -0.44340518  0.11480764  0.03780643]\n",
      " [-0.2687873  -0.904613    1.1977613   1.122406    0.94387585]\n",
      " [ 1.93788     0.33246467 -0.302842    1.7069211   0.377404  ]\n",
      " [ 0.3803016   0.285857    1.6468214  -0.15645935  0.8790724 ]\n",
      " [-0.62692213  0.6497635  -1.158973   -0.2130427   0.7091609 ]\n",
      " [ 0.75202245  0.23479185 -0.06548275  0.9145675  -0.4386912 ]\n",
      " [-1.4403553   1.2968105   0.08680099  0.16958351  1.0478038 ]\n",
      " [-0.38542104  1.2644668   0.5553166  -1.2104641   0.7246037 ]\n",
      " [-1.6519115  -0.02466917  0.5046274  -0.18270174 -0.6660187 ]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from quiz import get_weights, get_biases, linear\n",
    "\n",
    "\n",
    "def mnist_features_labels(n_labels):\n",
    "    \"\"\"\n",
    "    Gets the first <n> labels from the MNIST dataset\n",
    "    :param n_labels: Number of labels to use\n",
    "    :return: Tuple of feature list and label list\n",
    "    \"\"\"\n",
    "    mnist_features = []\n",
    "    mnist_labels = []\n",
    "\n",
    "    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)\n",
    "\n",
    "    # In order to make quizzes run faster, we're only looking at 10000 images\n",
    "    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):\n",
    "\n",
    "        # Add features and labels if it's for the first <n>th labels\n",
    "        if mnist_label[:n_labels].any():\n",
    "            mnist_features.append(mnist_feature)\n",
    "            mnist_labels.append(mnist_label[:n_labels])\n",
    "\n",
    "    return mnist_features, mnist_labels\n",
    "\n",
    "\n",
    "# Number of features (28*28 image is 784 features)\n",
    "n_features = 784\n",
    "# Number of labels\n",
    "n_labels = 3\n",
    "\n",
    "# Features and Labels\n",
    "features = tf.placeholder(tf.float32)\n",
    "labels = tf.placeholder(tf.float32)\n",
    "\n",
    "# Weights and Biases\n",
    "w = get_weights(n_features, n_labels)\n",
    "b = get_biases(n_labels)\n",
    "\n",
    "# Linear Function xW + b\n",
    "logits = linear(features, w, b)\n",
    "\n",
    "# Training data\n",
    "train_features, train_labels = mnist_features_labels(n_labels)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    # TODO: Initialize session variables\n",
    "    \n",
    "    # Softmax\n",
    "    prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Cross entropy\n",
    "    # This quantifies how far off the predictions were.\n",
    "    # You'll learn more about this in future lessons.\n",
    "    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)\n",
    "\n",
    "    # Training loss\n",
    "    # You'll learn more about this in future lessons.\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Rate at which the weights are changed\n",
    "    # You'll learn more about this in future lessons.\n",
    "    learning_rate = 0.08\n",
    "\n",
    "    # Gradient Descent\n",
    "    # This is the method used to train the model\n",
    "    # You'll learn more about this in future lessons.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Run optimizer and get loss\n",
    "    _, l = session.run(\n",
    "        [optimizer, loss],\n",
    "        feed_dict={features: train_features, labels: train_labels})\n",
    "\n",
    "# Print loss\n",
    "print('Loss: {}'.format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    x = tf.Variable(5)\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    print(sess.run(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
